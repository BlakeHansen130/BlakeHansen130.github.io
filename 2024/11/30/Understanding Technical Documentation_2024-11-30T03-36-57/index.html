

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="">
  <meta name="keywords" content="">
  
    <meta name="description" content="title: “Deep Learning Based Gesture Recognition on ESP32-S3: From Training to Deployment”date: 2024-11-30showAuthor: falseauthors:  - gao-jiaxuantags:  - ESP32-S3  - Deep Learning  - Gesture Recognit">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning Based Gesture Recognition on ESP32-S3: From Training to Deployment">
<meta property="og:url" content="https://blakehansen130.github.io/2024/11/30/Understanding%20Technical%20Documentation_2024-11-30T03-36-57/index.html">
<meta property="og:site_name">
<meta property="og:description" content="title: “Deep Learning Based Gesture Recognition on ESP32-S3: From Training to Deployment”date: 2024-11-30showAuthor: falseauthors:  - gao-jiaxuantags:  - ESP32-S3  - Deep Learning  - Gesture Recognit">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blakehansen130.github.io/img/fun/124719153_811927_p0.jpg">
<meta property="article:published_time" content="2024-11-30T03:36:57.273Z">
<meta property="article:modified_time" content="2024-11-30T05:42:46.631Z">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://blakehansen130.github.io/img/fun/124719153_811927_p0.jpg">
  
  
  
  <title>Deep Learning Based Gesture Recognition on ESP32-S3: From Training to Deployment - </title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  



  
<link rel="stylesheet" href="//at.alicdn.com/t/c/font_4749377_2bbylnq94i9.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"blakehansen130.github.io","root":"/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/fun/123096189_42399563_0.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Gao&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/" target="_self">
                <i class="iconfont icon-link-fill"></i>
                <span>友链</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/fun/124719594_9212166_p0.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Deep Learning Based Gesture Recognition on ESP32-S3: From Training to Deployment"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-11-30 11:36" pubdate>
          2024年11月30日 中午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          703 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          6 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">Deep Learning Based Gesture Recognition on ESP32-S3: From Training to Deployment</h1>
            
            
              <div class="markdown-body">
                
                <hr>
<h2 id="title-“Deep-Learning-Based-Gesture-Recognition-on-ESP32-S3-From-Training-to-Deployment”date-2024-11-30showAuthor-falseauthors-gao-jiaxuantags-ESP32-S3-Deep-Learning-Gesture-Recognition-ESP-DL-Model-Quantization"><a href="#title-“Deep-Learning-Based-Gesture-Recognition-on-ESP32-S3-From-Training-to-Deployment”date-2024-11-30showAuthor-falseauthors-gao-jiaxuantags-ESP32-S3-Deep-Learning-Gesture-Recognition-ESP-DL-Model-Quantization" class="headerlink" title="title: “Deep Learning Based Gesture Recognition on ESP32-S3: From Training to Deployment”date: 2024-11-30showAuthor: falseauthors:  - gao-jiaxuantags:  - ESP32-S3  - Deep Learning  - Gesture Recognition  - ESP-DL  - Model Quantization"></a>title: “Deep Learning Based Gesture Recognition on ESP32-S3: From Training to Deployment”<br>date: 2024-11-30<br>showAuthor: false<br>authors:<br>  - gao-jiaxuan<br>tags:<br>  - ESP32-S3<br>  - Deep Learning<br>  - Gesture Recognition<br>  - ESP-DL<br>  - Model Quantization</h2><p>Integrating deep learning capabilities into embedded systems has become a crucial aspect of modern IoT applications. Although powerful deep learning models can achieve high recognition accuracy, deploying these models on resource-constrained devices poses considerable challenges. This article presents a gesture recognition system based on the ESP32-S3, detailing the entire workflow from model training to deployment on embedded hardware. The complete project implementation and code are available at <a target="_blank" rel="noopener" href="https://github.com/BlakeHansen130/gesture-recognition-model">gesture-recognition-model</a>. By utilizing ESP-DL(master branch) and incorporating efficient quantization strategies with ESP-PPQ, this study demonstrates the feasibility of achieving real-time gesture recognition on resource-limited devices while maintaining satisfactory accuracy. Additionally, insights and methodologies were inspired by the work described in <a target="_blank" rel="noopener" href="https://developer.espressif.com/blog/hand-gesture-recognition-on-esp32-s3-with-esp-deep-learning/">Espressif’s blog on hand gesture recognition</a>, which significantly influenced the approach taken in this article.</p>
<p><strong><em>This article provides an overview of the complete development process for a gesture recognition system, encompassing dataset preparation, model training, and deployment.</em></strong></p>
<p><em>The content is organized into five main sections. The first section, “System Design,” outlines the overall system architecture and development environment. The second section, “Model Development,” addresses the design of the network architecture and the training strategy. The third section discusses techniques for “Quantization Optimization.” The fourth section focuses on “Resource-Constrained Deployment,” and the final section presents a comprehensive analysis of experimental results.</em></p>
<h2 id="System-Design"><a href="#System-Design" class="headerlink" title="System Design"></a>System Design</h2><p>The gesture recognition system is built upon the ESP32-S3 platform, leveraging its computational capabilities and memory resources for deep learning inference. The system architecture encompasses both hardware and software components, carefully designed to achieve optimal performance within the constraints of embedded deployment.</p>
<h3 id="Development-Environment"><a href="#Development-Environment" class="headerlink" title="Development Environment"></a>Development Environment</h3><p>The development process requires two distinct Conda environments to handle different stages of the workflow. The primary training environment, designated as ‘dl_env’, manages dataset preprocessing, model training, and basic evaluation tasks. A separate quantization environment, ‘esp-dl’, is specifically configured for model quantization, accuracy assessment, and ESP-DL format conversion.</p>
<p>For the deployment phase, ESP-IDF version 5.x is used, with specific testing conducted on v5.3.1. The implementation relies on the master branch of ESP-DL, which can be obtained through:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> -b v5.3.1 https://github.com/espressif/esp-idf.git<br>git <span class="hljs-built_in">clone</span> https://github.com/espressif/esp-dl.git<br></code></pre></td></tr></table></figure>

<p>It is important to note that using released versions of ESP-DL (such as idfv4.4 or v1.1) may not yield the same quantization performance as the latest master branch. The system configuration requires careful memory management, particularly in handling the ESP32-S3’s PSRAM. This is configured through the ESP-IDF menuconfig system with specific attention to flash size and SPIRAM settings.</p>
<h3 id="Memory-Management"><a href="#Memory-Management" class="headerlink" title="Memory Management"></a>Memory Management</h3><p>The system implements a sophisticated memory management strategy to optimize resource utilization. The CMake configuration is structured to properly integrate the ESP-DL library:</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs cmake"><span class="hljs-keyword">cmake_minimum_required</span>(VERSION <span class="hljs-number">3.5</span>)<br><br><span class="hljs-keyword">set</span>(EXTRA_COMPONENT_DIRS<br><span class="hljs-string">&quot;$ENV&#123;HOME&#125;/esp/esp-dl/esp-dl&quot;</span><br>)<br><br><span class="hljs-keyword">include</span>($ENV&#123;IDF_PATH&#125;/tools/cmake/<span class="hljs-keyword">project</span>.cmake)<br><span class="hljs-keyword">project</span>(gesture_recognition)<br></code></pre></td></tr></table></figure>
<p>__Note: Ensure that CMake can locate the esp-dl library cloned from GitHub by using relative paths to reference the esp-dl directory.</p>
<p>Component registration is handled through a dedicated CMakeLists.txt configuration:</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs cmake">idf_component_register(<br>SRCS<br><span class="hljs-string">&quot;app_main.cpp&quot;</span><br>INCLUDE_DIRS<br><span class="hljs-string">&quot;.&quot;</span><br><span class="hljs-string">&quot;model&quot;</span><br>REQUIRES<br>esp-dl<br>)<br></code></pre></td></tr></table></figure>

<p>The memory architecture prioritizes efficient PSRAM utilization, with model weights and input&#x2F;output tensors strategically allocated to optimize performance. Runtime monitoring of memory usage is implemented through built-in ESP-IDF functions:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-type">size_t</span> free_mem = <span class="hljs-built_in">heap_caps_get_free_size</span>(MALLOC_CAP_SPIRAM);<br><span class="hljs-built_in">ESP_LOGI</span>(TAG, <span class="hljs-string">&quot;Available PSRAM: %u bytes&quot;</span>, free_mem);<br></code></pre></td></tr></table></figure>

<p>This comprehensive system design forms the foundation for subsequent model development and deployment stages, ensuring robust performance within the constraints of embedded hardware. The careful consideration of memory management and environment configuration is crucial for successful deep learning deployment on resource-constrained devices.</p>
<h2 id="Model-Development"><a href="#Model-Development" class="headerlink" title="Model Development"></a>Model Development</h2><p>The gesture recognition model employs a lightweight architecture optimized for embedded deployment while maintaining high accuracy. Based on MobileNetV2’s inverted residual blocks, the network architecture, termed LightGestureNet, is specifically designed for efficient execution on the ESP32-S3 platform.</p>
<p>The model architecture processes grayscale images of size 96x96 pixels and classifies them into eight distinct gesture classes. The network structure begins with an initial convolutional layer, followed by a series of inverted residual blocks, and concludes with a classifier layer. The implementation details are illustrated in the following code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">first_layer = Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>)<br>inverted_residual_blocks = [<br>    (<span class="hljs-number">16</span>, <span class="hljs-number">24</span>, stride=<span class="hljs-number">2</span>, expand_ratio=<span class="hljs-number">6</span>),<br>    (<span class="hljs-number">24</span>, <span class="hljs-number">24</span>, stride=<span class="hljs-number">1</span>, expand_ratio=<span class="hljs-number">6</span>),<br>    (<span class="hljs-number">24</span>, <span class="hljs-number">32</span>, stride=<span class="hljs-number">2</span>, expand_ratio=<span class="hljs-number">6</span>),<br>    (<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, stride=<span class="hljs-number">1</span>, expand_ratio=<span class="hljs-number">6</span>)<br>]<br>classifier = Linear(<span class="hljs-number">32</span>, num_classes=<span class="hljs-number">8</span>)<br></code></pre></td></tr></table></figure>

<p>The initial convolutional layer processes single-channel grayscale input with 16 output channels and a stride of 2. The inverted residual blocks follow a systematic pattern of channel expansion and compression, with carefully chosen stride values to control spatial dimension reduction. The expand ratio of 6 in each block provides a balance between model capacity and computational efficiency.</p>
<p>The training process incorporates comprehensive data preprocessing and augmentation strategies. Input images undergo several transformations including resizing to 96x96 pixels, grayscale conversion, and normalization to the [0,1] range. Data augmentation techniques enhance model robustness through random rotation, scaling, and translation operations.</p>
<p>The training configuration utilizes the Adam optimizer with an initial learning rate of 0.001, implementing cosine annealing for learning rate decay. The following code demonstrates the model loading and inference process:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> model <span class="hljs-keyword">import</span> LightGestureNet<br><br>model = LightGestureNet()<br>model.load_state_dict(torch.load(<span class="hljs-string">&#x27;gesture_model.pth&#x27;</span>))<br>model.<span class="hljs-built_in">eval</span>()<br><br>input_tensor = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">96</span>, <span class="hljs-number">96</span>)<br>output = model(input_tensor)<br></code></pre></td></tr></table></figure>

<p>For deployment flexibility, the model supports multiple export formats. The ONNX format enables cross-platform inference capabilities, as demonstrated in the following implementation:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> onnxruntime<br><br>session = onnxruntime.InferenceSession(<span class="hljs-string">&#x27;gesture_model.onnx&#x27;</span>)<br><br>input_name = session.get_inputs()[<span class="hljs-number">0</span>].name<br>output = session.run(<span class="hljs-literal">None</span>, &#123;input_name: input_array&#125;)<br></code></pre></td></tr></table></figure>

<p>The ONNX runtime session initialization creates an optimized execution environment for the model. The inference process requires proper input name extraction and tensor formatting to ensure correct model execution.</p>
<p>The gesture classification system encompasses eight distinct classes, mapped through a standardized dictionary:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">CLASS_NAMES = &#123;<br>    <span class="hljs-number">0</span>: <span class="hljs-string">&#x27;palm&#x27;</span>,<br>    <span class="hljs-number">1</span>: <span class="hljs-string">&#x27;l&#x27;</span>,<br>    <span class="hljs-number">2</span>: <span class="hljs-string">&#x27;fist&#x27;</span>,<br>    <span class="hljs-number">3</span>: <span class="hljs-string">&#x27;thumb&#x27;</span>,<br>    <span class="hljs-number">4</span>: <span class="hljs-string">&#x27;index&#x27;</span>,<br>    <span class="hljs-number">5</span>: <span class="hljs-string">&#x27;ok&#x27;</span>,<br>    <span class="hljs-number">6</span>: <span class="hljs-string">&#x27;c&#x27;</span>,<br>    <span class="hljs-number">7</span>: <span class="hljs-string">&#x27;down&#x27;</span><br>&#125;<br></code></pre></td></tr></table></figure>

<p>This class mapping ensures consistent interpretation of model outputs across different deployment scenarios. Performance metrics demonstrate the effectiveness of the architecture, with training accuracy exceeding 99% and validation accuracy maintaining above 98%. The model size varies by format: PyTorch at 2MB, TFLite at 1.5MB, and ONNX at 2.2MB.</p>
<p>The model development phase establishes a robust foundation for subsequent quantization and deployment stages, achieving an optimal balance between recognition accuracy and computational efficiency within the constraints of embedded systems.</p>
<h2 id="Quantization-Optimization"><a href="#Quantization-Optimization" class="headerlink" title="Quantization Optimization"></a>Quantization Optimization</h2><p>Model quantization serves as a critical bridge between high-precision deep learning models and resource-constrained embedded systems. For the ESP32-S3 platform, three distinct quantization strategies have been developed and implemented, each offering unique advantages for different deployment scenarios.</p>
<h3 id="INT8-Quantization-Implementation"><a href="#INT8-Quantization-Implementation" class="headerlink" title="INT8 Quantization Implementation"></a>INT8 Quantization Implementation</h3><p>The baseline quantization method implements uniform 8-bit quantization across all layers. This fundamental approach provides a solid starting point for model compression:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> onnxruntime<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> ppq <span class="hljs-keyword">import</span> *<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">quantize_int8</span>(<span class="hljs-params">model</span>):<br><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;cal.pkl&#x27;</span>, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        X_cal, _ = pickle.load(f)<br>    <br>    settings = QuantizationSettingFactory.default_setting()<br>    settings.quantize_parameter_setting.bit_width = <span class="hljs-number">8</span><br>    settings.quantize_parameter_setting.symmetrical = <span class="hljs-literal">True</span><br>    settings.quantize_parameter_setting.per_channel = <span class="hljs-literal">True</span><br><br>    quantum = quantize_torch_model(<br>        model=model,<br>        calib_dataloader=X_cal,<br>        setting=settings,<br>        platform=TargetPlatform.PPL_CUDA_INT8<br>    )<br>    <br>    <span class="hljs-keyword">return</span> quantum<br></code></pre></td></tr></table></figure>

<p>In this implementation, the calibration data plays a crucial role in determining optimal quantization parameters. The <code>bit_width</code> parameter sets the quantization precision to 8 bits, while <code>symmetrical=True</code> ensures the quantization scheme maintains zero at the center of the range. The <code>per_channel=True</code> setting enables more fine-grained quantization by treating each channel independently, which helps preserve model accuracy.</p>
<h3 id="Mixed-Precision-Strategy"><a href="#Mixed-Precision-Strategy" class="headerlink" title="Mixed Precision Strategy"></a>Mixed Precision Strategy</h3><p>The mixed precision approach provides more flexibility by allowing different quantization precisions for different layers based on their sensitivity to quantization:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">mixed_precision_quantize</span>(<span class="hljs-params">model</span>):<br><br>    precision_config = &#123;<br>        <span class="hljs-string">&#x27;conv1&#x27;</span>: &#123;<span class="hljs-string">&#x27;w_bits&#x27;</span>: <span class="hljs-number">8</span>, <span class="hljs-string">&#x27;a_bits&#x27;</span>: <span class="hljs-number">8</span>&#125;,<br>        <span class="hljs-string">&#x27;conv2&#x27;</span>: &#123;<span class="hljs-string">&#x27;w_bits&#x27;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&#x27;a_bits&#x27;</span>: <span class="hljs-number">8</span>&#125;,<br>        <span class="hljs-string">&#x27;fc&#x27;</span>: &#123;<span class="hljs-string">&#x27;w_bits&#x27;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&#x27;a_bits&#x27;</span>: <span class="hljs-number">16</span>&#125;<br>    &#125;<br>    <br>    settings = QuantizationSettingFactory.mixed_precision_setting()<br>    settings.optimization_level = <span class="hljs-number">1</span><br>    <br>    <span class="hljs-keyword">for</span> layer, config <span class="hljs-keyword">in</span> precision_config.items():<br>        settings.quantize_parameter_setting[layer] = LayerQuantSetting(<br>            w_bits=config[<span class="hljs-string">&#x27;w_bits&#x27;</span>],<br>            a_bits=config[<span class="hljs-string">&#x27;a_bits&#x27;</span>]<br>        )<br>    <br>    quantum = quantize_torch_model(<br>        model=model,<br>        calib_dataloader=X_cal,<br>        setting=settings,<br>        platform=TargetPlatform.PPL_CUDA_MIX<br>    )<br>    <br>    <span class="hljs-keyword">return</span> quantum<br></code></pre></td></tr></table></figure>

<p>This strategy allows precise control over the quantization of each layer. The <code>precision_config</code> dictionary defines both weight (<code>w_bits</code>) and activation (<code>a_bits</code>) precision for each layer. Early layers and critical feature extraction components often benefit from higher precision, while later layers can typically tolerate more aggressive quantization. The optimization level parameter controls how aggressively the quantization algorithm searches for optimal bit assignments.</p>
<h3 id="Equalization-Aware-Quantization"><a href="#Equalization-Aware-Quantization" class="headerlink" title="Equalization-Aware Quantization"></a>Equalization-Aware Quantization</h3><p>The equalization-aware approach introduces an advanced quantization method that considers the distribution of values across layers:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">equalization_quantize</span>(<span class="hljs-params">model</span>):<br>    settings = QuantizationSettingFactory.default_setting()<br>    settings.equalization = <span class="hljs-literal">True</span><br>    settings.equalization_setting.iterations = <span class="hljs-number">4</span><br>    settings.equalization_setting.value_threshold = <span class="hljs-number">0.4</span><br>    settings.optimization_level = <span class="hljs-number">2</span><br>    <br>    model = convert_relu6_to_relu(model)<br><br>    quantum = quantize_torch_model(<br>        model=model,<br>        calib_dataloader=X_cal,<br>        setting=settings,<br>        platform=TargetPlatform.PPL_CUDA_INT8<br>    )<br>    <br>    <span class="hljs-keyword">return</span> quantum<br></code></pre></td></tr></table></figure>

<p>The equalization process iteratively adjusts scaling factors across layers to minimize quantization error. The <code>iterations</code> parameter controls how many times the equalization algorithm runs, while <code>value_threshold</code> determines when the algorithm considers values significant enough to influence scaling decisions. The conversion from ReLU6 to ReLU is necessary for compatibility with the equalization process, as the capped activation function can interfere with proper scale determination.</p>
<h3 id="Performance-Evaluation"><a href="#Performance-Evaluation" class="headerlink" title="Performance Evaluation"></a>Performance Evaluation</h3><p>To assess the effectiveness of each quantization strategy, a comprehensive evaluation framework has been implemented:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_quantized_model</span>(<span class="hljs-params">model, test_data</span>):<br>    metrics = &#123;&#125;<br><br>    metrics[<span class="hljs-string">&#x27;size&#x27;</span>] = get_model_size(model)<br><br>    predictions = model.predict(test_data)<br>    metrics[<span class="hljs-string">&#x27;accuracy&#x27;</span>] = calculate_accuracy(predictions, test_labels)<br><br>    metrics[<span class="hljs-string">&#x27;latency&#x27;</span>] = measure_inference_time(model, test_data[<span class="hljs-number">0</span>])<br>    <br>    <span class="hljs-keyword">return</span> metrics<br></code></pre></td></tr></table></figure>

<p>This evaluation framework examines three critical aspects of each quantized model: memory footprint, accuracy retention, and inference latency. The modular design allows for easy comparison between different quantization strategies and helps in selecting the most appropriate approach for specific deployment requirements.</p>
<p>Each quantization method presents its own set of trade-offs, and the choice between them depends on specific application requirements such as memory constraints, accuracy requirements, and inference speed needs. The systematic implementation and evaluation of these strategies ensure optimal deployment on the ESP32-S3 platform.</p>
<h2 id="Resource-Constrained-Deployment"><a href="#Resource-Constrained-Deployment" class="headerlink" title="Resource-Constrained Deployment"></a>Resource-Constrained Deployment</h2><p>The deployment phase of the gesture recognition system on ESP32-S3 requires careful consideration of hardware constraints and system configurations. The implementation focuses on optimal memory utilization and efficient inference execution while maintaining system stability.</p>
<h3 id="Memory-and-Partition-Configuration"><a href="#Memory-and-Partition-Configuration" class="headerlink" title="Memory and Partition Configuration"></a>Memory and Partition Configuration</h3><p>The ESP32-S3’s memory architecture necessitates specific configurations through the ESP-IDF menuconfig system. The following memory settings are crucial for optimal model deployment:</p>
<figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs arduino"><span class="hljs-built_in">Serial</span> Flasher Configuration<br>└── Flash Size: <span class="hljs-number">8</span>MB<br>Component Configuration<br>└── ESP PSRAM<br>    ├── Support <span class="hljs-keyword">for</span> external, <span class="hljs-built_in">SPI</span>-connected RAM<br>    ├── <span class="hljs-built_in">SPI</span> RAM config<br>    │   ├── SPIRAM_MODE: Octal<br>    │   ├── SPIRAM_SPEED: <span class="hljs-number">40</span> MHz<br>    │   └── Enable <span class="hljs-built_in">SPI</span> RAM during startup<br>    └── Allow .bss Segment Placement in PSRAM: Enabled<br></code></pre></td></tr></table></figure>

<p>These configurations enable efficient utilization of external RAM for model weight storage and inference operations. The partition table configuration requires customization to accommodate the model data:</p>
<figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs mathematica"><span class="hljs-built_in">Partition</span> <span class="hljs-built_in">Table</span><br>└── <span class="hljs-built_in">Partition</span> <span class="hljs-built_in">Table</span><span class="hljs-operator">:</span> <span class="hljs-variable">Custom</span> <span class="hljs-built_in">Partition</span> <span class="hljs-built_in">Table</span> <span class="hljs-variable">CSV</span><br>└── <span class="hljs-variable">Custom</span> <span class="hljs-built_in">Partition</span> <span class="hljs-variable">CSV</span> <span class="hljs-built_in">File</span><span class="hljs-operator">:</span> <span class="hljs-variable">partitions</span><span class="hljs-operator">.</span><span class="hljs-variable">csv</span><br></code></pre></td></tr></table></figure>

<p>The custom partition layout ensures adequate space allocation for both the application and model data, facilitating efficient model loading during runtime.</p>
<h3 id="Build-System-Integration"><a href="#Build-System-Integration" class="headerlink" title="Build System Integration"></a>Build System Integration</h3><p>The project’s CMake configuration integrates the ESP-DL framework and establishes proper component relationships. The build system setup requires careful attention to include paths and dependencies:</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs cmake">idf_component_register(<br>    SRCS<br>        <span class="hljs-string">&quot;app_main.cpp&quot;</span><br>    INCLUDE_DIRS<br>        <span class="hljs-string">&quot;.&quot;</span><br>        <span class="hljs-string">&quot;model&quot;</span><br>    REQUIRES<br>        esp-dl<br>)<br></code></pre></td></tr></table></figure>

<p>This configuration ensures proper compilation of the application components and correct linking with the ESP-DL framework. The integration of model files and headers follows a systematic approach through the build system.</p>
<h3 id="Runtime-Memory-Management"><a href="#Runtime-Memory-Management" class="headerlink" title="Runtime Memory Management"></a>Runtime Memory Management</h3><p>The implementation employs strategic memory allocation to optimize resource utilization during model inference. Memory monitoring capabilities are integrated into the system to ensure stable operation:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-type">size_t</span> free_mem = <span class="hljs-built_in">heap_caps_get_free_size</span>(MALLOC_CAP_SPIRAM);<br><span class="hljs-built_in">ESP_LOGI</span>(TAG, <span class="hljs-string">&quot;Available PSRAM: %u bytes&quot;</span>, free_mem);<br></code></pre></td></tr></table></figure>

<p>Model weights are stored in PSRAM while keeping critical runtime buffers in internal RAM for optimal performance. The system implements memory monitoring to prevent allocation failures and maintain stable operation during continuous inference.</p>
<h3 id="Serial-Communication-Configuration"><a href="#Serial-Communication-Configuration" class="headerlink" title="Serial Communication Configuration"></a>Serial Communication Configuration</h3><p>The deployment setup includes specific serial communication parameters for reliable device interaction:</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs routeros">Component<span class="hljs-built_in"> config</span><br><span class="hljs-built_in"></span>└── ESP<span class="hljs-built_in"> System Settings</span><br><span class="hljs-built_in"></span>    └── Channel <span class="hljs-keyword">for</span><span class="hljs-built_in"> console </span>output<br>        └── Default: UART0<br><br>Component<span class="hljs-built_in"> config</span><br><span class="hljs-built_in"></span>└── Bluetooth<br>    └── NimBLE Options<br>        └── Host-controller Transport<br>            └── <span class="hljs-built_in">Enable</span> Uart Transport<br>                └── Uart Hci Baud Rate: 115200<br><br></code></pre></td></tr></table></figure>

<p>These settings ensure stable communication during both development and deployment phases. The baud rate selection balances reliable communication with flash programming speed.</p>
<h3 id="Driver-Installation-and-System-Integration"><a href="#Driver-Installation-and-System-Integration" class="headerlink" title="Driver Installation and System Integration"></a>Driver Installation and System Integration</h3><p>The deployment process requires proper USB driver installation for device communication. The CH340&#x2F;CH341 driver installation process follows system-specific procedures, ensuring reliable device connectivity.</p>
<h2 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h2><p>The gesture recognition system on ESP32-S3 showed strong performance across multiple metrics. Both quantitative and qualitative evaluations were conducted.</p>
<p>The baseline model (prior to quantization) established high accuracy across eight gesture classes, particularly distinguishing distinct gestures like ‘palm’ and ‘fist’. This floating-point model served as a benchmark for subsequent optimizations.</p>
<p>After quantization, the mixed-precision approach balanced resource use and recognition accuracy effectively. INT8 quantization minimized model size, while selectively using 16-bit precision in key layers maintained feature discrimination. Equalization-aware quantization helped keep consistent performance across all gestures.</p>
<p>On the ESP32-S3, the optimized model achieved real-time inference with moderate power consumption and efficient memory use, leveraging both internal RAM and PSRAM effectively. These results confirm that the optimization strategies are suitable for deploying advanced gesture recognition on resource-constrained platforms, enabling practical real-time applications.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol>
<li>ESP official resources</li>
</ol>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/espressif/esp-dl">ESP-DL main repository and documents</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/espressif/esp-idf">ESP-IDF development framework</a></li>
<li><a target="_blank" rel="noopener" href="https://www.espressif.com/sites/default/files/documentation/esp32-s3_technical_reference_manual_en.pdf">ESP32-S3 Technical Reference Manual</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.espressif.com/projects/esp-idf/en/latest/esp32s3/">ESP-IDF Programming Guide</a></li>
</ul>
<ol start="2">
<li>ESP-PPQ toolchain</li>
</ol>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/espressif/esp-ppq/">ESP-PPQ project</a> Contains:<ul>
<li>Core API</li>
<li>Quantizer documentation</li>
<li>Executor implementation</li>
<li>Usage guide</li>
</ul>
</li>
</ul>
<ol start="3">
<li>Tutorials and guides</li>
</ol>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/alibukharai/Blogs/tree/main/ESP-DL">ESP-DL Complete Workflow</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/espressif/esp-dl/blob/master/tutorial/how_to_load_model_cn.md">Model Loading Tutorial</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/espressif/esp-dl/blob/master/tutorial/how_to_deploy_mobilenet_v2_cn.md">MobileNet V2 Deployment Guide</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/espressif/esp-dl/blob/master/tutorial/how_to_quantize_model_cn.md">Model Quantization Guide</a></li>
</ul>
<ol start="4">
<li>Datasets</li>
</ol>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/gti-upm/leapgestrecog">LeapGestRecog Dataset</a></li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/fyp/" class="category-chain-item">fyp</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Deep Learning Based Gesture Recognition on ESP32-S3: From Training to Deployment</div>
      <div>https://blakehansen130.github.io/2024/11/30/Understanding Technical Documentation_2024-11-30T03-36-57/</div>
    </div>
    <div class="license-meta">
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年11月30日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/11/29/development-environment-docs/" title="手势识别项目环境配置文档">
                        <span class="hidden-mobile">手势识别项目环境配置文档</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
